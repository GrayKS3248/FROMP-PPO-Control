# -*- coding: utf-8 -*-
"""
Created on Tue Mar  9 16:11:39 2021

@author: GKSch
"""

import torch
from CNN import Model as cnn
import numpy as np
import matplotlib.pyplot as plt
import pickle
import os
import pandas as pd

class Autoencoder:
    
    # Initializes an object of the autoencoder class. This class compresses input images to a linear 128 element latent representation 
    # via a compressed form of the AlexNet CNN. This class requires that input images and target images have the same dimensions.
    # @param Learning rate
    # @param Exponential learning rate decay
    # @param Path from which a previous model will be loaded
    # @param First dimension of the input and target images
    # @param Second dimension of the input and target images
    # @param Minimum value of input. Used for normalization
    # @param Maximum value of input. Used for normalization
    # @param The number of targets reconstructed by the autoencoder from the input image
    # @param The number of latent representation targets generated by the encoder from the input image
    # @param Probability of noise being added to the input image before convolution
    # @param Stdev of white noise added to the input image before convolution
    # @param Probability of occulsion being added to the input image before convolution
    # @param Maximum number of occulsions added to the input image before convolution
    # @param Maximum total normalized area of all occulsions added to the input image before convolution
    # @param Verbose
    def __init__(self, alpha, decay, load_path="", dim_1=0, dim_2=0, norm_min=0.0, norm_max=1.0, num_targets=0, num_latent=0, sparsity_parameter=0.05, sparsity_const=0.0, image_const=1.0, latent_const=1.0, noise_prob=0.0, noise_stdev=0.0, occ_prob=0.0, occ_max=1, occ_min_area=0.0, occ_max_area=0.0, verbose=True):
        
        # Initialize hyperparameters
        self.sparsity_parameter = sparsity_parameter
        self.sparsity_const = sparsity_const
        self.image_const = image_const
        self.latent_const = latent_const
        self.alpha_zero = alpha
        self.alpha_decay = decay
        self.noise_prob = noise_prob
        self.noise_stdev = noise_stdev
        self.occ_prob = occ_prob
        self.occ_max = occ_max
        self.occ_min_area = occ_min_area
        self.occ_max_area = occ_max_area
        if noise_stdev==0.0:
            self.noisy = False
        else:
            self.noisy = True
        
        # Initialize or load model
        if (load_path==""):
            
            # Store NN size parameters
            self.dim_1 = dim_1
            self.dim_2 = dim_2
            self.norm_min = norm_min
            self.norm_max = norm_max
            self.num_targets = num_targets
            self.num_latent = num_latent
            
            # Initialize model
            self.model = cnn(np.max((self.dim_1, self.dim_2)), self.num_targets, self.num_latent)
            
            # Initialize buffer for training curve and lr curve
            self.loss_curve = []
            self.lr_curve = []
            self.sparsity_curve = []
            
            # Initialize variables to track snapshots during training
            self.batch_num = 0
            self.input_snapshots = [[], [], []]
            self.images_targets_snapshots = [[], [], []]
            if self.num_latent >= 1:
                self.latent_targets_snapshots = [[], [], []]
                self.output_latent_snapshots = [[], [], []]
            self.output_images_snapshots = [[], [], []]
            self.loss_snapshots = [[], [], []]
            self.batch_num_snapshots = []
       
        else:
            self.load(load_path, verbose=verbose)
            
        # Initialize optimizer, scheduler, and criterion
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.alpha_zero)
        self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=self.optimizer, gamma=self.alpha_decay)
        self.criterion_BCE = torch.nn.BCELoss()
        self.criterion_MSE = torch.nn.MSELoss()
        
        # Load model onto GPU
        self.device = self.get_device()
        self.model.to(self.device)
        
        # User readout
        if verbose:
            print("Autoencoder Device(")
            print("  " + self.device)
            print(")\n")
            print("Autoencoder " + str(self.model))
            print("\nAutoencoder Parameters(")
            print("  (Dimensions): " + str(self.dim_1) + "x" + str(self.dim_2))
            print("  (Num Targets): " + str(self.num_targets))
            print("  (Num Latent): " + str(self.num_latent))
            print("  (Noise Stdev): {0:.3f}".format(self.noise_stdev))
            print(")\n")
        
    # Loads a saved autoencoder at path/output
    # @param Path from which the autoencoder will be loaded
    def load(self, load_path, verbose=True):
        
        # Copy NN at path to current module
        if verbose:
            print("Loading: " + load_path + "/output \n")
        if not os.path.isdir(load_path):
            raise RuntimeError("Could not find " + load_path)
        else:
            with open(load_path+"/output", 'rb') as file:
                loaded_data = pickle.load(file)

            # Load network size parameters
            self.dim_1 = loaded_data['dim_1']
            self.dim_2 = loaded_data['dim_2']
            self.norm_min = loaded_data['norm_min']
            self.norm_max = loaded_data['norm_max']
            self.num_targets = loaded_data['num_targets']
            self.num_latent = loaded_data['num_latent']
                
            # Load model parameters
            self.model = cnn(np.max((self.dim_1, self.dim_2)), self.num_targets, self.num_latent)
            self.model.load_state_dict(loaded_data['model'].state_dict())
            
            # Initialize buffer for training curve and lr curve
            self.loss_curve = loaded_data['loss_curve']
            self.lr_curve = loaded_data['lr_curve']
            self.sparsity_curve = loaded_data['sparsity_curve']
            
            # Initialize snapshot variables
            self.batch_num = loaded_data['batch_num']
            self.input_snapshots = loaded_data['input_snapshots']
            self.images_targets_snapshots = loaded_data['images_targets_snapshots']
            if self.num_latent >= 1:
                self.latent_targets_snapshots = loaded_data['latent_targets_snapshots']
                self.output_latent_snapshots = loaded_data['output_latent_snapshots']
            self.output_images_snapshots = loaded_data['output_images_snapshots']
            self.batch_num_snapshots = loaded_data['batch_num_snapshots']
            self.loss_snapshots = loaded_data['loss_snapshots']
    
    # Gets the cpu or gpu on which to run
    # @return device code
    def get_device(self):
        if torch.cuda.is_available():
            device = 'cuda:0'
        else:
            device = 'cpu'
        return device
 
    # Converts any non square input image into a square input image via linear interpolation about the shortest axis
    # @param The input image to be converted
    # @return The squared input image
    def convert_to_square(self, input_image):
        
        # Extract dimensions
        input_image = np.array(input_image)
        largest_dim = max(input_image.shape)
        smallest_dim = min(input_image.shape)
        smallest_ax = np.argmin(input_image.shape)
        new_image = np.zeros((largest_dim, largest_dim))
        new_image_mask = np.zeros(smallest_dim)
        start_point_of_interpolant = 0
        
        # Extract information regarding the interpolation
        length_of_interpolant = largest_dim // (smallest_dim - 1)
        num_of_interpolants_one_longer = largest_dim - (length_of_interpolant * (smallest_dim - 1)) - 1
        spacing_on_longer_interpolants = (smallest_dim - 1) / num_of_interpolants_one_longer
        indicies_of_longer_interpolants = np.round(np.arange(0, smallest_dim - 1, spacing_on_longer_interpolants))
        
        # Interpolate
        for curr_interpolant_index in range(input_image.shape[smallest_ax]-1):
            if (indicies_of_longer_interpolants == curr_interpolant_index).any():
                length_of_curr_interpolant = length_of_interpolant+2
            else:
               length_of_curr_interpolant = length_of_interpolant+1
            curr_interpolant = np.transpose(np.linspace( input_image[:,curr_interpolant_index], input_image[:,curr_interpolant_index+1], length_of_curr_interpolant) )
            new_image[:,start_point_of_interpolant:start_point_of_interpolant+length_of_curr_interpolant] = curr_interpolant
            new_image_mask[curr_interpolant_index] = start_point_of_interpolant
            start_point_of_interpolant = start_point_of_interpolant + length_of_curr_interpolant-1
        
        # Complete last entry of image mask
        new_image_mask[-1] = largest_dim - 1
        
        return new_image, new_image_mask
    
    # Normalizes input image so that the min is 0 and max is 1
    # @param The input image to be normalized
    # @return The normalized input image
    def normalize(self, input_image):
        
        new_image = np.zeros(input_image.shape)
        
        # Static range normalization
        if self.norm_min != self.norm_max:
            new_image = (input_image - self.norm_min) / (self.norm_max - self.norm_min)
        else:
            new_image = input_image / self.norm_min
        
        # Normalized range assurance
        new_image[new_image>1.0] = 1.0
        new_image[new_image<0.0] = 0.0
        
        return new_image

    # Adds white noise to image based on initialization function noise stdev.
    # @param The input image to which noise is added
    # @return The noisy input image
    def add_noise(self, input_image):
        if self.noisy and ((np.random.rand() - self.noise_prob) <= 0.0):
            norm_input_image = self.normalize(input_image)
            noisy_norm_input_image = norm_input_image + np.random.normal(0.0, np.random.rand()*self.noise_stdev, input_image.shape)
            noisy_norm_input_image[noisy_norm_input_image>1.0]=1.0
            noisy_norm_input_image[noisy_norm_input_image<0.0]=0.0
            noisy_image = self.denormalize(noisy_norm_input_image)
        else:
            noisy_image = input_image
            
        return noisy_image
    
    # Adds randomly shaped and sized occulsions to image.
    # @param The input image to which occulsions are added
    # @return The occulded input image
    def add_occulsion(self, input_image):
        
        if (np.random.rand() - self.occ_prob) <= 0.0:
            
            # Determine the number of occulsions and thier total area
            num_occulsions = np.random.randint(1,self.occ_max+1)
            total_area = np.random.rand()*(self.occ_max_area - self.occ_min_area) + self.occ_min_area
            
            # Determine the area of each occulsion
            if num_occulsions == 1:
                area_list = [ total_area ]
            else:
                area_list = [ np.random.rand() * total_area ]
                for i in range(num_occulsions-2):
                    area_used_so_far = np.sum(area_list[0:i+1])
                    area_list.append( np.random.rand()*(total_area - area_used_so_far) )
                area_list.append( total_area - np.sum(area_list) )
            if abs(np.sum(area_list) - total_area) > 1e-5:
                raise  ValueError("Occuluded area checksum error.")
            if abs(len(area_list) - num_occulsions) != 0:
                raise  ValueError("Number of occulusions checksum error.")
                
            # Generate occulsion centers, types, and dimensions
            occ_cen = []
            occ_type = []
            occ_dim = []
            occ_val = []
            for i in range(num_occulsions):
                occ_cen.append([np.random.rand()*np.sqrt(self.dim_1/self.dim_2),np.random.rand()*np.sqrt(self.dim_1/self.dim_2)*(self.dim_2/self.dim_1)])
                occ_val.append(np.random.rand())
                if (np.random.rand() >= 0.50):
                     occ_type.append("Ellipse")
                     axis_1 = 0.5 * np.random.rand() * np.sqrt(self.dim_1/self.dim_2)
                     axis_2 = area_list[i] / (np.pi * axis_1)
                     occ_dim.append([axis_1, axis_2])
                else:
                     occ_type.append("Rectangle")
                     axis_1 = 0.5 * np.random.rand() * np.sqrt(self.dim_1/self.dim_2)
                     axis_2 = area_list[i] / axis_1
                     occ_dim.append([axis_1, axis_2])
                     
            # Apply occulsions to input image
            occuluded_image = self.normalize(input_image)
            for i in range(self.dim_1):
                for j in range(self.dim_2):
                    
                    generalized_coord_1 = (i/(self.dim_1-1)) * (np.sqrt(self.dim_1/self.dim_2))
                    generalized_coord_2 = (j/(self.dim_2-1)) * (np.sqrt(self.dim_1/self.dim_2)*(self.dim_2/self.dim_1))
                    
                    for k in range(num_occulsions):
                        if occ_type[k] == "Ellipse":
                            if (generalized_coord_1-occ_cen[k][0])**2.0 / (occ_dim[k][0])**2.0 + (generalized_coord_2-occ_cen[k][1])**2.0 / (occ_dim[k][1])**2.0 <= 1.0:
                                occuluded_image[i][j] = occ_val[k] + (2.0*np.random.rand()-1.0)*0.10
                                
                        elif occ_type[k] == "Rectangle":
                            if generalized_coord_1 >= occ_cen[k][0]-0.50*occ_dim[k][0] and generalized_coord_1 <= occ_cen[k][0]+0.50*occ_dim[k][0]:
                                if generalized_coord_2 >= occ_cen[k][1]-0.50*occ_dim[k][1] and generalized_coord_2 <= occ_cen[k][1]+0.50*occ_dim[k][1]:
                                    occuluded_image[i][j] = occ_val[k] + (2.0*np.random.rand()-1.0)*0.10
                                
            occuluded_image = self.denormalize(occuluded_image)
                                
        else:
            occuluded_image = input_image
        
        return occuluded_image
    
    # Denormalizes output image so that the mean and stdev match the original image
    # @param The output image to be denormalized
    # @return The denormalized output image
    def denormalize(self, output_image):
        new_image = output_image * (self.norm_max - self.norm_min) + self.norm_min
            
        return new_image
 
    # Converts any squared output image back to its original dimensions via linear regression along the shortest axis
    # @param The squared output image to be converted
    # @param Mask indicating the interpolant index at each point of square output image
    # @return The output image in its original dimensions
    def convert_to_orig_dim(self, output_image, sq_image_mask):
        
        # Create linear regression across each super sample and grab intercept values
        new_image = np.zeros((self.dim_1, self.dim_2))
        for i in range(len(sq_image_mask) - 1):
            x_coords = np.arange(sq_image_mask[i], sq_image_mask[i+1]+1) - sq_image_mask[i]
            y_coords = output_image[:,np.int(sq_image_mask[i]):np.int(sq_image_mask[i+1]+1)]
            A = np.vstack([x_coords, np.ones(len(x_coords))]).T
            slope_intercept = np.matmul(np.matmul(np.linalg.inv(np.matmul(A.T,A)),A.T),y_coords.T)
            if i == 0:
                new_image[:, i] = slope_intercept[1]
            else:
                new_image[:, i] = 0.5 * (new_image[:, i] + slope_intercept[1])
            new_image[:,i+1] = slope_intercept[1] + slope_intercept[0] * x_coords[-1]
            
        return new_image   
 
    # Forward propagates an input image through the autoencoder
    # @param The input image to be propogated
    # @return The propogated output images
    def forward(self, input_image):
        
        # Convert input to proper data type
        with torch.no_grad():
            
            # Square and normalize input
            sq_input_image, sq_image_mask = self.convert_to_square(input_image)
            norm_sq_input_image = self.normalize(sq_input_image)
                
            # Convert to tensor of correct dimensions
            norm_sq_input_image = torch.tensor(norm_sq_input_image)
            norm_sq_input_image = norm_sq_input_image.reshape(1,1,norm_sq_input_image.shape[0],norm_sq_input_image.shape[1]).float()
            norm_sq_input_image = norm_sq_input_image.to(self.device)
            
            # Forward through model
            if self.num_latent >= 1:
                norm_sq_output_image, code_sparsity, latent_vars = self.model.forward(norm_sq_input_image)
                code_sparsity = code_sparsity.to('cpu').squeeze().numpy().mean()
                latent_vars = latent_vars.to('cpu').squeeze().numpy()
                norm_sq_output_image = norm_sq_output_image.to('cpu').squeeze().numpy()
            else:
                norm_sq_output_image, code_sparsity = self.model.forward(norm_sq_input_image)
                code_sparsity = code_sparsity.to('cpu').squeeze().numpy().mean()
                norm_sq_output_image = norm_sq_output_image.to('cpu').squeeze().numpy()
            
            # Return to original distribution and dimensions
            output_images = []
            for target in range(self.num_targets):
                if self.num_targets == 1:
                    sq_output_image = self.denormalize(norm_sq_output_image)
                else: 
                    sq_output_image = self.denormalize(norm_sq_output_image[target])
                output_images.append(self.convert_to_orig_dim(sq_output_image, sq_image_mask))
            output_images = np.array(output_images)
                
        # Return the encoded frame of the proper data type
        if self.num_latent >= 1:
            return output_images, code_sparsity, latent_vars
        else:
            return output_images, code_sparsity
       
    # Encodes a given input image
    # @param The input image to be encoded
    # @return The linear latent representation
    def encode(self, input_image):

        # Convert input to proper data type
        with torch.no_grad():
            
            # Square and normalize input
            sq_input_image, sq_image_mask = self.convert_to_square(input_image)
            norm_sq_input_image = self.normalize(sq_input_image)
            
            # Convert to tensor of correct dimensions
            norm_sq_input_image = torch.tensor(norm_sq_input_image)
            norm_sq_input_image = norm_sq_input_image.reshape(1,1,norm_sq_input_image.shape[0],norm_sq_input_image.shape[1]).float()
            norm_sq_input_image = norm_sq_input_image.to(self.device)
            
            # Forward through model
            if self.num_latent >= 1:
                latent_representation, latent_vars = self.model.encode(norm_sq_input_image)
                latent_representation = latent_representation.to('cpu').squeeze().numpy()
                latent_vars = latent_vars.to('cpu').squeeze().numpy()
            else:
                latent_representation = self.model.encode(norm_sq_input_image).to('cpu').squeeze().numpy()
        
        # Return the encoded frame of the proper data type
        if self.num_latent >= 1:
            return latent_representation, latent_vars
        else:
            return latent_representation
    
    # Generates a candidate image given a latent representation
    # @param The latent seed of generation
    # @return The generated image
    def generate(self, latent_seed):

        # Convert input to proper data type
        with torch.no_grad():
            
            # Convert to tensor of correct dimensions
            latent_seed = torch.tensor(latent_seed)
            latent_seed = latent_seed.reshape(1,latent_seed.shape[0]).float()
            latent_seed = latent_seed.to(self.device)
            
            # Forward through model
            norm_sq_output_image = self.model.generate(latent_seed).to('cpu').squeeze().numpy()
        
            # Return to original distribution and dimensions
            output_images = []
            _, sq_image_mask = self.convert_to_square(np.random.rand(self.dim_1,self.dim_2))
            for target in range(self.num_targets):
                if self.num_targets == 1:
                    sq_output_image = self.denormalize(norm_sq_output_image)
                else: 
                    sq_output_image = self.denormalize(norm_sq_output_image[target])
                output_images.append(self.convert_to_orig_dim(sq_output_image, sq_image_mask))
            output_images = np.array(output_images)
                
        # Return the encoded frame of the proper data type
        return output_images
    
    # Performs one epoch of stochastic gradient descent. ALL INPUT AND TARGET IMAGES MUST USE SCALE
    # @param Full batch of input images
    # @param Full batch of target images
    # @return Average training loss over entire batch
    def learn(self, input_batch, targets_batch, take_snapshot=False):
        
        # Snapshot mechanics
        if take_snapshot:
            loss_buffer = []
            input_buffer = []
            latent_targets_buffer = []
            images_targets_buffer = []
        
        # Format all batch data
        batch_length = len(input_batch[:,0,0])
        formatted_input_batch = np.zeros((batch_length, 1, max(self.dim_1,self.dim_2), max(self.dim_1,self.dim_2)))
        formatted_target_batch = np.zeros((batch_length, self.num_targets, max(self.dim_1,self.dim_2), max(self.dim_1,self.dim_2)))
        formatted_latent_batch = np.zeros((batch_length, self.num_latent))
        coding_sparsity_target = self.sparsity_parameter * torch.ones(self.model.bottleneck, requires_grad=False).to(self.device)
        for i in range(batch_length):
            
            #Convert input and target batch to proper format, add noise + occulsions
            input_image = input_batch[i,:,:]
            targets_latent = []
            targets_images = []
            for j in range(len(targets_batch)):
                if isinstance(targets_batch[j][i], float) or isinstance(targets_batch[j][i], int):
                    targets_latent.append(targets_batch[j][i])
                elif len(np.array(targets_batch[j][i]).shape)==2:
                    if np.array(targets_batch[j][i]).shape[0] != self.dim_1:
                        raise ValueError("Target " + str(j) + " of batch trajectory " + str(i) + " has invalid dimensions. Expected ("+str(self.dim_1)+", "+str(self.dim_2)+") but got " + str(np.array(targets_batch[j][i]).shape))
                    if np.array(targets_batch[j][i]).shape[1] != self.dim_2:
                        raise ValueError("Target " + str(j) + " of batch trajectory " + str(i) + " has invalid dimensions. Expected ("+str(self.dim_1)+", "+str(self.dim_2)+") but got " + str(np.array(targets_batch[j][i]).shape))
                    targets_images.append(np.array(targets_batch[j][i]))
                else:
                    raise ValueError("Targets batch at index " + str(j) + " has invalid dimensions. Targets must be scalar or 2D numpy array or list.")
            if len(targets_latent) != self.num_latent:
                raise ValueError("Targets batch did not include the correct number of latent targets. Expected " + str(self.num_latent) + " but got " + str(len(targets_latent)))
            if len(targets_images) != self.num_targets:
                raise ValueError("Targets batch did not include the correct number of target images. Expected " + str(self.num_targets) + " but got " + str(len(targets_images)))
            
            # Square and normalize targets
            targets_images = np.array(targets_images)
            norm_sq_targets = np.zeros((self.num_targets, max(self.dim_1,self.dim_2), max(self.dim_1,self.dim_2)))
            for j in range(self.num_targets):
                sq_targets, _ = self.convert_to_square(targets_images[j,:,:])
                norm_sq_targets[j,:,:] = self.normalize(sq_targets)
            formatted_target_batch[i] = norm_sq_targets
            
            # Add latent variables to batch
            targets_latent = np.array(targets_latent)
            formatted_latent_batch[i] = targets_latent
            
            # Square, normalize, add noise to, and occlude input
            noisy_input_image = self.add_noise(input_image)
            noisy_occulded_input_image = self.add_occulsion(noisy_input_image)
            noisy_occluded_sq_input_image, _ = self.convert_to_square(noisy_occulded_input_image)
            noisy_occluded_norm_sq_input_image = self.normalize(noisy_occluded_sq_input_image)
            formatted_input_batch[i][0] = noisy_occluded_norm_sq_input_image
            
            # Snapshot mechanics
            if take_snapshot:
                input_buffer.append(noisy_occulded_input_image)
                latent_targets_buffer.append(targets_latent)
                images_targets_buffer.append(targets_images)
            
        # Convert inputs and targets to tensors
        formatted_input_batch = torch.tensor(formatted_input_batch, requires_grad=True).float().to(self.device)
        formatted_target_batch = torch.tensor(formatted_target_batch, requires_grad=False).float().to(self.device)
        formatted_latent_batch = torch.tensor(formatted_latent_batch, requires_grad=False).float().to(self.device)
            
        # Forward propogate through model
        if self.num_latent >= 1:
            formatted_output_batch, code_sparsity, formatted_output_latent_batch = self.model.forward(formatted_input_batch)
        else:
            formatted_output_batch, code_sparsity = self.model.forward(formatted_input_batch)
              
        # Snapshot mechanics
        if take_snapshot:
            with torch.no_grad():
                sparsity_loss = ((coding_sparsity_target*(coding_sparsity_target/code_sparsity).log()) + ((1.0-coding_sparsity_target)*((1.0-coding_sparsity_target)/(1.0-code_sparsity)).log())).sum()
                
                # Calculate input-wise loss
                for i in range(batch_length):
                    if self.num_latent >= 1:
                        image_loss = self.criterion_BCE(formatted_output_batch[i], formatted_target_batch[i])
                        latent_loss = self.criterion_MSE(formatted_output_latent_batch[i], formatted_latent_batch[i])
                        loss = self.sparsity_const*sparsity_loss + self.image_const*image_loss + self.latent_const*latent_loss
                    else:
                        image_loss = self.criterion_BCE(formatted_output_batch[i], formatted_target_batch[i])
                        loss = self.sparsity_const*sparsity_loss + self.image_const*image_loss
                    loss_buffer.append(loss.item())
            
        # Calculate batch-wise loss
        if self.num_latent >= 1:
            sparsity_loss = ((coding_sparsity_target*(coding_sparsity_target/code_sparsity).log()) + ((1.0-coding_sparsity_target)*((1.0-coding_sparsity_target)/(1.0-code_sparsity)).log())).sum()
            image_loss = self.criterion_BCE(formatted_output_batch, formatted_target_batch)
            latent_loss = self.criterion_MSE(formatted_output_latent_batch, formatted_latent_batch)
            if np.isinf(sparsity_loss.item()) or np.isnan(sparsity_loss.item()):
                loss = self.image_const*image_loss + self.latent_const*latent_loss
            else:
                loss = self.sparsity_const*sparsity_loss + self.image_const*image_loss + self.latent_const*latent_loss
        else:
            sparsity_loss = ((coding_sparsity_target*(coding_sparsity_target/code_sparsity).log()) + ((1.0-coding_sparsity_target)*((1.0-coding_sparsity_target)/(1.0-code_sparsity)).log())).sum()
            image_loss = self.criterion_BCE(formatted_output_batch, formatted_target_batch)
            if np.isinf(sparsity_loss.item()) or np.isnan(sparsity_loss.item()):
                loss = self.image_const*image_loss
            else:
                loss = self.sparsity_const*sparsity_loss + self.image_const*image_loss
            
        if np.isnan(loss.item()):
            if self.num_latent >= 1:
                print(str(sparsity_loss))
                print(str(image_loss))
                print(str(latent_loss))
                print(str(loss))
            else:
                print(str(sparsity_loss))
                print(str(image_loss))
                print(str(loss))
            wait = input("Press Enter to continue.")
            
        # Store lr and loss data
        self.lr_curve.append(self.lr_scheduler.get_last_lr()[0])
        self.loss_curve.append(loss.item())
        self.sparsity_curve.append(code_sparsity.mean().item())
        
        # Take optimization step
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        self.lr_scheduler.step()

        # Snapshot mechanics
        self.batch_num = self.batch_num + 1
        if take_snapshot:
            index_of_min_loss = np.argmin(loss_buffer)
            index_of_avg_loss = np.argmin(abs(np.array(loss_buffer) - np.mean(loss_buffer)))
            index_of_max_loss = np.argmax(loss_buffer)
            self.input_snapshots[0].append(input_buffer[index_of_min_loss])
            self.input_snapshots[1].append(input_buffer[index_of_avg_loss])
            self.input_snapshots[2].append(input_buffer[index_of_max_loss])
            self.images_targets_snapshots[0].append(images_targets_buffer[index_of_min_loss])
            self.images_targets_snapshots[1].append(images_targets_buffer[index_of_avg_loss])
            self.images_targets_snapshots[2].append(images_targets_buffer[index_of_max_loss])
            if self.num_latent >= 1:
                self.latent_targets_snapshots[0].append(latent_targets_buffer[index_of_min_loss])
                self.latent_targets_snapshots[1].append(latent_targets_buffer[index_of_avg_loss])
                self.latent_targets_snapshots[2].append(latent_targets_buffer[index_of_max_loss])
                output_images_0, code_sparsity_0, latent_vars_0 = self.forward(input_buffer[index_of_min_loss])
                output_images_1, code_sparsity_1, latent_vars_1 = self.forward(input_buffer[index_of_avg_loss])
                output_images_2, code_sparsity_2, latent_vars_2 = self.forward(input_buffer[index_of_max_loss])
                self.output_latent_snapshots[0].append(latent_vars_0)
                self.output_latent_snapshots[1].append(latent_vars_1)
                self.output_latent_snapshots[2].append(latent_vars_2)
                self.output_images_snapshots[0].append(output_images_0)
                self.output_images_snapshots[1].append(output_images_1)
                self.output_images_snapshots[2].append(output_images_2)
            else:
                output_images_0, code_sparsity_0 = self.forward(input_buffer[index_of_min_loss])
                output_images_1, code_sparsity_1 = self.forward(input_buffer[index_of_avg_loss])
                output_images_2, code_sparsity_2 = self.forward(input_buffer[index_of_max_loss])
                self.output_images_snapshots[0].append(output_images_0)
                self.output_images_snapshots[1].append(output_images_1)
                self.output_images_snapshots[2].append(output_images_2)
            self.loss_snapshots[0].append(np.round(loss_buffer[index_of_min_loss],3))
            self.loss_snapshots[1].append(np.round(loss_buffer[index_of_avg_loss],3))
            self.loss_snapshots[2].append(np.round(loss_buffer[index_of_max_loss],3))
            self.batch_num_snapshots.append(self.batch_num)
        
        # Return the average RMS reconstruction error
        return self.loss_curve[-1], self.lr_curve[-1], self.sparsity_curve[-1]
    
    # Saves the training data and trained model
    # @param Boolean flag indicating to draw loss curve and learning rate curve for all training
    # @param Boolean flag indicating to render snapshots taken during all training
    def save(self, draw=True, render=True):
        print("\nSaving autoencoder results...")

        # Store data to dictionary
        if self.num_latent >= 1:
            data = {
                "dim_1" : self.dim_1,
                "dim_2" : self.dim_2,
                "norm_min" : self.norm_min,
                "norm_max" : self.norm_max,
                "num_targets" : self.num_targets,
                "num_latent" : self.num_latent,
                "loss_curve" : self.loss_curve,
                "lr_curve" : self.lr_curve,
                "sparsity_curve" : self.sparsity_curve,
                "model" : self.model.to('cpu'),
                "batch_num" : self.batch_num,
                "input_snapshots" : self.input_snapshots,
                "images_targets_snapshots" : self.images_targets_snapshots,
                "latent_targets_snapshots" : self.latent_targets_snapshots,
                "output_latent_snapshots" : self.output_latent_snapshots,
                "output_images_snapshots" : self.output_images_snapshots,
                "batch_num_snapshots" : self.batch_num_snapshots,
                "loss_snapshots" : self.loss_snapshots,
            }
        else:
            data = {
                "dim_1" : self.dim_1,
                "dim_2" : self.dim_2,
                "norm_min" : self.norm_min,
                "norm_max" : self.norm_max,
                "num_targets" : self.num_targets,
                "num_latent" : self.num_latent,
                "loss_curve" : self.loss_curve,
                "lr_curve" : self.lr_curve,
                "sparsity_curve" : self.sparsity_curve,
                "model" : self.model.to('cpu'),
                "batch_num" : self.batch_num,
                "input_snapshots" : self.input_snapshots,
                "images_targets_snapshots" : self.images_targets_snapshots,
                "output_images_snapshots" : self.output_images_snapshots,
                "batch_num_snapshots" : self.batch_num_snapshots,
                "loss_snapshots" : self.loss_snapshots,
            }
        self.model.to(self.device)

        # Find save paths
        curr_dir_num = 1
        path = "../results/AE_" + str(curr_dir_num)
        done = False
        while not done:
            if not os.path.isdir(path):
                os.mkdir(path)
                done = True
            else:
                curr_dir_num = curr_dir_num + 1
                path = "../results/AE_" + str(curr_dir_num)

        # Pickle all important outputs
        save_file = path + "/output"
        with open(save_file, 'wb') as file:
            pickle.dump(data, file)
        self.path = path
        
        # Draw and render
        if draw:
            self.draw()
        if render:
            self.render()

    # Draws and saves the loss and learning rate curves
    def draw(self):
        print("Plotting autoencoder training curve...")
        
        # Get the moving average and stdev of the learning curve
        window = len(self.loss_curve) // 600
        if window > 1:
            rolling_avg = np.array(pd.Series(self.loss_curve).rolling(window).mean())
            rolling_avg = rolling_avg[~np.isnan(rolling_avg)]
            rolling_avg_min = np.floor(min(rolling_avg)*10.0)/10.0
            rolling_avg_max = np.ceil(max(rolling_avg)*10.0)/10.0
            ticks = np.arange(rolling_avg_min, rolling_avg_max+0.1, 0.1)
            lables = ["{:.0e}".format(item) for item in ticks]
            
            # Draw training curve with rolling values
            plt.clf()
            plt.title("Rolling Average Loss, Window = " + str(window) + " Batches",fontsize='xx-large')
            plt.xlabel("Batch",fontsize='large')
            plt.ylabel("BCE Loss",fontsize='large')
            plt.ylim([rolling_avg_min,rolling_avg_max])
            plt.yscale("log")
            plt.plot(np.array([*range(len(rolling_avg))])+window,rolling_avg,lw=2.5,c='r')
            plt.xticks(fontsize='large')
            plt.yticks(ticks=ticks, labels=lables, fontsize='large')
            plt.gcf().set_size_inches(8.5, 5.5)
            save_file = self.path + "/rolling_loss.png"
            plt.savefig(save_file, dpi = 500)
            plt.close()
        
        # Trim data
        inds = [*range(len(self.loss_curve))]
        if len(inds) > 10000:
            spacing = round(len(self.loss_curve) / 10000)
            tunc_loss_curve = self.loss_curve[0:-1:spacing]
            trunc_lr_curve = self.lr_curve[0:-1:spacing]
            trunc_sparsity_curve = self.sparsity_curve[0:-1:spacing]
            trunc_inds = inds[0:-1:spacing]
            if trunc_inds[-1] != inds[-1]:
                tunc_loss_curve.append(self.loss_curve[-1])
                trunc_lr_curve.append(self.lr_curve[-1])
                trunc_sparsity_curve.append(self.sparsity_curve[-1])
                trunc_inds.append(inds[-1])
        else:
            tunc_loss_curve = self.loss_curve
            trunc_lr_curve = self.lr_curve
            trunc_sparsity_curve = self.sparsity_curve
            trunc_inds = inds
        
        # Draw training curve
        plt.clf()
        plt.title("Loss Curve",fontsize='xx-large')
        plt.xlabel("Batch",fontsize='large')
        plt.ylabel("MSE",fontsize='large')
        plt.plot(trunc_inds,tunc_loss_curve,lw=2.5,c='r')
        plt.yscale("log")
        plt.xticks(fontsize='large')
        plt.yticks(fontsize='large')
        plt.gcf().set_size_inches(8.5, 5.5)
        save_file = self.path + "/loss.png"
        plt.savefig(save_file, dpi = 500)
        plt.close()
        
        # Draw learning rate curve
        plt.clf()
        plt.title("Learning Rate",fontsize='xx-large')
        plt.xlabel("Batch",fontsize='large')
        plt.ylabel("Learning Rate",fontsize='large')
        plt.plot(trunc_inds,trunc_lr_curve,lw=2.5,c='r')
        plt.xticks(fontsize='large')
        plt.yticks(fontsize='large')
        plt.gcf().set_size_inches(8.5, 5.5)
        save_file = self.path + "/lr.png"
        plt.savefig(save_file, dpi = 500)
        plt.close()
        
        # Draw sparsity curve
        plt.clf()
        plt.title("Coding Sparsity",fontsize='xx-large')
        plt.xlabel("Batch",fontsize='large')
        plt.ylabel("Probability of Coding Neuron Activation",fontsize='large')
        plt.plot(trunc_inds,trunc_sparsity_curve,lw=2.5,c='r')
        plt.axhline(y=self.sparsity_parameter,lw=2.5,c='k',ls="--")
        plt.xticks(fontsize='large')
        plt.yticks(fontsize='large')
        plt.gcf().set_size_inches(8.5, 5.5)
        save_file = self.path + "/sparsity.png"
        plt.savefig(save_file, dpi = 500)
        plt.close()
    
    # Renders snapshots taken during training to single folder
    def render(self):
        print("Rendering...")
        
        # Find save paths
        if not os.path.isdir(self.path + "/video"):
            os.mkdir(self.path + "/video")
            
        # Create grids
        x_grid, y_grid = np.meshgrid(np.linspace(0,1,self.dim_1), np.linspace(0,1,self.dim_2))
            
        for snapshot in range(len(self.batch_num_snapshots)):
                   
            # Minimum loss figure
            plt.cla()
            plt.clf()
            fig = plt.figure(constrained_layout=True, figsize=(8, 8))
            subfigs = fig.subfigures(3, 1)
            input_ax = subfigs[0].subplots(1,1)
            target_axs = subfigs[1].subplots(1, self.num_targets)
            output_axs = subfigs[2].subplots(1, self.num_targets)
            
            # Minimum loss input image
            input_ax.pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.input_snapshots[0][snapshot])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
            input_ax.tick_params(axis='x',labelsize=12)
            input_ax.tick_params(axis='y',labelsize=12)
            input_ax.set_aspect(0.25, adjustable='box')
            input_ax.set_title('Input',fontsize='x-large')
            
            # Minimum loss target image
            if self.num_targets == 1:
                target_axs.pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.images_targets_snapshots[0][snapshot][0])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                target_axs.tick_params(axis='x',labelsize=12)
                target_axs.tick_params(axis='y',labelsize=12)
                target_axs.set_aspect(0.25, adjustable='box')
                target_axs.set_title('Target',fontsize='x-large')
            else:
                for target in range(self.num_targets):
                    target_axs[target].pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.images_targets_snapshots[0][snapshot][target])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                    target_axs[target].tick_params(axis='x',labelsize=12)
                    target_axs[target].tick_params(axis='y',labelsize=12)
                    target_axs[target].set_aspect(0.25, adjustable='box')
                    target_axs[target].set_title('Target '+str(target+1),fontsize='x-large')
                
            # Minimum loss target supertitle string
            latent_target_string = ""
            if self.num_latent==1:
               latent_target_string = latent_target_string + 'Latent Target = ' + '{:.3f}'.format(float(self.latent_targets_snapshots[0][snapshot]))
            elif self.num_latent>1:
                latent_target_string = latent_target_string + 'Latent Target = ('
                for i in range(self.num_latent):
                    latent_target_string = latent_target_string + '{:.3f}'.format(self.latent_targets_snapshots[0][snapshot][i])
                    if i != self.num_latent-1:
                        latent_target_string = latent_target_string + ", "
                    else:
                        latent_target_string = latent_target_string + ")"
            
            # Minimum loss output image
            if self.num_targets == 1:
                output_axs.pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.output_images_snapshots[0][snapshot][0])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                output_axs.tick_params(axis='x',labelsize=12)
                output_axs.tick_params(axis='y',labelsize=12)
                output_axs.set_aspect(0.25, adjustable='box')
                output_axs.set_title('Output',fontsize='x-large')
            else:
                for output in range(self.num_targets):
                    output_axs[output].pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.output_images_snapshots[0][snapshot][output])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                    output_axs[output].tick_params(axis='x',labelsize=12)
                    output_axs[output].tick_params(axis='y',labelsize=12)
                    output_axs[output].set_aspect(0.25, adjustable='box')
                    output_axs[output].set_title('Output '+str(output+1),fontsize='x-large')
            
            # Minimum loss output supertitle string
            latent_output_string = ""
            if self.num_latent==1:
               latent_output_string = latent_output_string + 'Latent Output = ' + '{:.3f}'.format(float(self.output_latent_snapshots[0][snapshot]))
            elif self.num_latent>1:
                latent_output_string = latent_output_string + 'Latent Output = ('
                for i in range(self.num_latent):
                    latent_output_string = latent_output_string + '{:.3f}'.format(self.output_latent_snapshots[0][snapshot][i])
                    if i != self.num_latent-1:
                        latent_output_string = latent_output_string + ", "
                    else:
                        latent_output_string = latent_output_string + ")"
            
            # Save and close minimum loss figure
            if self.num_latent >= 1:
                plt.suptitle('Batch ' + str(self.batch_num_snapshots[snapshot])+"  |  Loss = " + '{:.3f}'.format(self.loss_snapshots[0][snapshot]) + "\n" + latent_target_string + "\n" + latent_output_string + "\n",fontsize='xx-large')
            else:
                plt.suptitle('Batch ' + str(self.batch_num_snapshots[snapshot])+"  |  Loss = " + '{:.3f}'.format(self.loss_snapshots[0][snapshot]) + "\n",fontsize='xx-large')
            plt.savefig(self.path + "/video/min_in_"+str(snapshot).zfill(3)+'.png', dpi=100)
            plt.close()
            
            # Average loss figure
            plt.cla()
            plt.clf()
            fig = plt.figure(constrained_layout=True, figsize=(8, 8))
            subfigs = fig.subfigures(3, 1)
            input_ax = subfigs[0].subplots(1,1)
            target_axs = subfigs[1].subplots(1, self.num_targets)
            output_axs = subfigs[2].subplots(1, self.num_targets)
            
            # Average loss input image
            input_ax.pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.input_snapshots[1][snapshot])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
            input_ax.tick_params(axis='x',labelsize=12)
            input_ax.tick_params(axis='y',labelsize=12)
            input_ax.set_aspect(0.25, adjustable='box')
            input_ax.set_title('Input',fontsize='x-large')
            
            # Average loss target image
            if self.num_targets == 1:
                target_axs.pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.images_targets_snapshots[1][snapshot][0])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                target_axs.tick_params(axis='x',labelsize=12)
                target_axs.tick_params(axis='y',labelsize=12)
                target_axs.set_aspect(0.25, adjustable='box')
                target_axs.set_title('Target',fontsize='x-large')
            else:
                for target in range(self.num_targets):
                    target_axs[target].pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.images_targets_snapshots[1][snapshot][target])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                    target_axs[target].tick_params(axis='x',labelsize=12)
                    target_axs[target].tick_params(axis='y',labelsize=12)
                    target_axs[target].set_aspect(0.25, adjustable='box')
                    target_axs[target].set_title('Target '+str(target+1),fontsize='x-large')

            # Average loss target supertitle string
            latent_target_string = ""
            if self.num_latent==1:
               latent_target_string = latent_target_string + 'Latent Target = ' + '{:.3f}'.format(float(self.latent_targets_snapshots[1][snapshot]))
            elif self.num_latent>1:
                latent_target_string = latent_target_string + 'Latent Target = ('
                for i in range(self.num_latent):
                    latent_target_string = latent_target_string + '{:.3f}'.format(self.latent_targets_snapshots[1][snapshot][i])
                    if i != self.num_latent-1:
                        latent_target_string = latent_target_string + ", "
                    else:
                        latent_target_string = latent_target_string + ")"
                
            # Average loss output image
            if self.num_targets == 1:
                output_axs.pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.output_images_snapshots[1][snapshot][0])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                output_axs.tick_params(axis='x',labelsize=12)
                output_axs.tick_params(axis='y',labelsize=12)
                output_axs.set_aspect(0.25, adjustable='box')
                output_axs.set_title('Output',fontsize='x-large')
            else:
                for output in range(self.num_targets):
                    output_axs[output].pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.output_images_snapshots[1][snapshot][output])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                    output_axs[output].tick_params(axis='x',labelsize=12)
                    output_axs[output].tick_params(axis='y',labelsize=12)
                    output_axs[output].set_aspect(0.25, adjustable='box')
                    output_axs[output].set_title('Output '+str(target+1),fontsize='x-large')
            
            # Average loss output supertitle string
            latent_output_string = ""
            if self.num_latent==1:
               latent_output_string = latent_output_string + 'Latent Output = ' + '{:.3f}'.format(float(self.output_latent_snapshots[1][snapshot]))
            elif self.num_latent>1:
                latent_output_string = latent_output_string + 'Latent Output = ('
                for i in range(self.num_latent):
                    latent_output_string = latent_output_string + '{:.3f}'.format(self.output_latent_snapshots[1][snapshot][i])
                    if i != self.num_latent-1:
                        latent_output_string = latent_output_string + ", "
                    else:
                        latent_output_string = latent_output_string + ")"
            
            # Save and close average loss figure
            if self.num_latent >= 1:
                plt.suptitle('Batch ' + str(self.batch_num_snapshots[snapshot])+"  |  Loss = " + '{:.3f}'.format(self.loss_snapshots[1][snapshot]) + "\n" + latent_target_string + "\n" + latent_output_string + "\n",fontsize='xx-large')
            else:
                plt.suptitle('Batch ' + str(self.batch_num_snapshots[snapshot])+"  |  Loss = " + '{:.3f}'.format(self.loss_snapshots[1][snapshot]) + "\n",fontsize='xx-large')
            plt.savefig(self.path + "/video/avg_in_"+str(snapshot).zfill(3)+'.png', dpi=100)
            plt.close()
            
            # Maximum loss figure
            plt.cla()
            plt.clf()
            fig = plt.figure(constrained_layout=True, figsize=(8, 8))
            subfigs = fig.subfigures(3, 1)
            input_ax = subfigs[0].subplots(1,1)
            target_axs = subfigs[1].subplots(1, self.num_targets)
            output_axs = subfigs[2].subplots(1, self.num_targets)
            
            # Maximum loss input image
            input_ax.pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.input_snapshots[2][snapshot])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
            input_ax.tick_params(axis='x',labelsize=12)
            input_ax.tick_params(axis='y',labelsize=12)
            input_ax.set_aspect(0.25, adjustable='box')
            input_ax.set_title('Input',fontsize='x-large')
            
            # Maximum loss target image
            if self.num_targets == 1:
                target_axs.pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.images_targets_snapshots[2][snapshot][0])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                target_axs.tick_params(axis='x',labelsize=12)
                target_axs.tick_params(axis='y',labelsize=12)
                target_axs.set_aspect(0.25, adjustable='box')
                target_axs.set_title('Target',fontsize='x-large')
            else:
                for target in range(self.num_targets):
                    target_axs[target].pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.images_targets_snapshots[2][snapshot][target])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                    target_axs[target].tick_params(axis='x',labelsize=12)
                    target_axs[target].tick_params(axis='y',labelsize=12)
                    target_axs[target].set_aspect(0.25, adjustable='box')
                    target_axs[target].set_title('Target '+str(target+1),fontsize='x-large')
            
            # Maximum loss target supertitle string
            latent_target_string = ""
            if self.num_latent==1:
               latent_target_string = latent_target_string + 'Latent Target = ' + '{:.3f}'.format(float(self.latent_targets_snapshots[2][snapshot]))
            elif self.num_latent>1:
                latent_target_string = latent_target_string + 'Latent Target = ('
                for i in range(self.num_latent):
                    latent_target_string = latent_target_string + '{:.3f}'.format(self.latent_targets_snapshots[2][snapshot][i])
                    if i != self.num_latent-1:
                        latent_target_string = latent_target_string + ", "
                    else:
                        latent_target_string = latent_target_string + ")"
            
            # Maximum loss output image
            if self.num_targets == 1:
                output_axs.pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.output_images_snapshots[2][snapshot][0])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                output_axs.tick_params(axis='x',labelsize=12)
                output_axs.tick_params(axis='y',labelsize=12)
                output_axs.set_aspect(0.25, adjustable='box')
                output_axs.set_title('Output',fontsize='x-large')
            else:
                for output in range(self.num_targets):
                    output_axs[output].pcolormesh(x_grid, y_grid, np.transpose(self.normalize(self.output_images_snapshots[2][snapshot][output])), shading='nearest', cmap='jet', vmin=0.0, vmax=1.0)
                    output_axs[output].tick_params(axis='x',labelsize=12)
                    output_axs[output].tick_params(axis='y',labelsize=12)
                    output_axs[output].set_aspect(0.25, adjustable='box')
                    output_axs[output].set_title('Output '+str(target+1),fontsize='x-large')
            
            # Maximum loss output supertitle string
            latent_output_string = ""
            if self.num_latent==1:
               latent_output_string = latent_output_string + 'Latent Output = ' + '{:.3f}'.format(float(self.output_latent_snapshots[2][snapshot]))
            elif self.num_latent>1:
                latent_output_string = latent_output_string + 'Latent Output = ('
                for i in range(self.num_latent):
                    latent_output_string = latent_output_string + '{:.3f}'.format(self.output_latent_snapshots[2][snapshot][i])
                    if i != self.num_latent-1:
                        latent_output_string = latent_output_string + ", "
                    else:
                        latent_output_string = latent_output_string + ")"
            
            # Save and close maximum loss figure
            if self.num_latent >= 1:
                plt.suptitle('Batch ' + str(self.batch_num_snapshots[snapshot])+"  |  Loss = " + '{:.3f}'.format(self.loss_snapshots[2][snapshot]) + "\n" + latent_target_string + "\n" + latent_output_string + "\n",fontsize='xx-large')
            else:
                plt.suptitle('Batch ' + str(self.batch_num_snapshots[snapshot])+"  |  Loss = " + '{:.3f}'.format(self.loss_snapshots[2][snapshot]) + "\n",fontsize='xx-large')
            plt.savefig(self.path + "/video/max_in_"+str(snapshot).zfill(3)+'.png', dpi=100)
            plt.close()